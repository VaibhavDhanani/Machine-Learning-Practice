what i learn from house price prediction problem ? 
--> new type of Regressor called as LightGBM
    --> 1. New Type of Regressor: LightGBM
        -->   LightGBM (Light Gradient Boosting Machine) is a powerful and efficient gradient boosting framework designed for performance and          scalability. It is particularly effective for structured/tabular data.
        Key Features of LightGBM:
        --> Handles large datasets efficiently by using a histogram-based algorithm.
        --> Supports categorical features directly, reducing the need for one-hot encoding.
        --> Faster training speed and lower memory usage compared to other boosting algorithms like XGBoost.
        --> Built-in feature importance ranking.
    --> Learning Outcome: LightGBM is a go-to algorithm for many Kaggle competitions and real-world scenarios where prediction accuracy is crucial. It is a good starting point for ensemble methods.


Feature	                            LightGBM         	                             Linear Regressor
Algorithm	              Gradient Boosting (Decision Trees)	                 Linear Statistical Model
Data Relationship	          Linear + Non-linear	                                   Only Linear
Feature Interactions	           Automatic 	                                         Manual
Complexity Handling	         Handles complex datasets well	              Struggles with complex datasets
Training Speed	          Slower (More resource-intensive)   	                         Faster
Interpretability	                  Low	                                              High
Overfitting Risk	          Higher (requires tuning)	                                  Lower
Best Use Cases	           Complex, large datasets	Simple,                     linearly separable data


--> LabelEncoding of categorical data because of its ordinality
    --> Ordinality in Data: If the categories have a natural order (e.g., "low" < "medium" < "high"), Label Encoding retains this order and allows the model to learn patterns based on it.
    --> Caution: When there's no inherent order in the categories (e.g., "red", "blue", "green"), Label Encoding may mislead the model. In such cases, other techniques like One-Hot Encoding are preferred.
Learning Outcome: Understanding when to use Label Encoding vs. One-Hot Encoding is essential for preprocessing categorical data effectively.


--> R² Score Interpretation
The R² (coefficient of determination) score measures the proportion of variance in the target variable that the model explains. Its range is:
1.0: Perfect prediction.
0.0: Model does no better than the mean of the target.
< 0.0: Model performs worse than simply predicting the mean.
Formula:
R^2=1− Total Sum of Squares (TSS) / Sum of Squared Residuals (SSR)

 
A high R² score indicates that the model captures the dataset well and predicts accurately. However, beware of overfitting when R² is extremely high on training data but low on test data.
Learning Outcome: Use R² as a baseline metric, but complement it with other evaluation metrics (e.g., RMSE, MAE) for a holistic view of model performance.
